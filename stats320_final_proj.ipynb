{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM6wZOvaLBidCfhiseAsx7d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## EM Algorithm\n","\n","- **Observables**: $x, y, \\hat z$\n","- **Latent**: $z$\n","- **Model**:\n","    - $p(z)$: prior over latent $z$\n","    - $p(\\hat z | z)$: observation model for $\\hat z$\n","    - $p(y|z,x,\\theta)$: likelihood model for $y$ parameterized by $\\theta$ and conditioned on $z$ and $x$\n","\n","The marginal likelihood is\n","\\begin{align}\n","p(y|x,\\hat z,\\theta) &= \\int p(y,z|x,\\hat z,\\theta) dz\\\\\n","&= \\frac{\\int p(y|z,x,\\theta)p(\\hat z|z)p(z)dz}{\\int p(\\hat z|z)p(z)dz}\n","\\end{align}\n","\n","- **E-step**: Infer latent variable $z$ based on current guess of parameters:\n","$$q(z) \\leftarrow p(z|x,y,\\hat z,\\theta) \\propto p(z)p(y,\\hat z|z,\\theta,x) = p(y|z,x,\\theta)p(\\hat z|z)p(z)$$\n","<!-- - **M-step**: $\\theta = \\text{argmax}_\\theta\\mathbb{E}_{q(z)}[p(y|z,x,\\theta)p(\\hat z|z)]$ -->\n","- **M-step**: Update parameters $\\theta$ to maximize the expected log-likelihood of the observed data under that inferred $q(z)$:\n","$$\\theta = \\text{argmax}_\\theta\\mathbb{E}_{q(z)}[\\log p(y|z,x,\\theta)]$$"],"metadata":{"id":"pq1zNzEYAmdG"}},{"cell_type":"code","source":["# Given:\n","# function model_lkhd(x, y, z, theta) = p(y|z,x,theta)\n","# grid size = [H, W]"],"metadata":{"id":"WQ3tvdvS6_es"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.distributions import MultivariateNormal"],"metadata":{"id":"eFW8X_JHd0TG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWylG3pwAig4"},"outputs":[],"source":["def E_step(x, y, z_hat, theta, z_sample, sigma=1.0):\n","    \"\"\"\n","    E-step: Infer z based on current guess of parameters\n","\n","    Parameters:\n","    x, y     : stimulus and observed neural response, inputs to model_lkhd\n","    z_hat    : torch tensor, observed eye trace location, shape [2]\n","    theta    : model parameters\n","    z_sample : np.ndarray of shape [H, W], count of times each pixel was sampled\n","    sigma    : float, std deviation for Gaussian likelihood p(z_hat | z)\n","\n","    Returns:\n","    q_z : np.ndarray of shape [H, W], posterior probability of sampling at each pixel\n","    \"\"\"\n","    # H, W = z_sample.shape\n","    # cov = torch.eye(2) * sigma**2\n","    # mvn = MultivariateNormal(z_hat, cov)\n","\n","    # q_z = np.zeros((H, W))\n","    # total_samples = z_sample.sum()\n","    # for h in range(H):\n","    #     for w in range(W):\n","    #         count = z_sample[h, w]\n","    #         if count == 0:\n","    #             continue\n","    #         else:\n","    #             z = torch.tensor([h, w])\n","    #             p_z_hat_given_z = torch.exp(mvn.log_prob(z)).item()\n","    #             lkhd = model_lkhd(x, y, z, theta)\n","    #             q_z[h, w] = lkhd * p_z_hat_given_z * count\n","    # q_z /= q_z.sum()\n","    # return q_z\n","\n","    H, W = z_sample.shape\n","    h_coords, w_coords = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n","    z_grid = torch.stack([h_coords, w_coords], dim=-1).reshape(-1, 2)  # [H*W,2]\n","    z_sample_flat = torch.tensor(z_sample).flatten()  # [N]\n","\n","    # Mask only sampled pixels\n","    mask = z_sample_flat > 0  # [H*W]\n","    z_grid_masked = z_grid[mask]  # [M,2]\n","    sample_weights = z_sample_flat[mask]  # [M]\n","    total_samples = sample_weights.sum()\n","\n","    mvn = MultivariateNormal(z_hat, torch.eye(2) * sigma**2)\n","    log_p_z_hat_given_z = mvn.log_prob(z_grid_masked)\n","    log_lkhd = model_log_lkhd(x, y, z_grid_masked, theta)\n","    log_q_z = log_lkhd + log_p_z_hat_given_z + sample_weights.log()\n","    q_z_unnorm = torch.exp(log_q_z - log_q_z.max())\n","\n","    q_flat = torch.zeros_like(z_sample_flat)\n","    q_flat[mask] = q_z_unnorm\n","    q_z = q_flat.reshape(H, W)\n","    q_z /= q_z.sum()\n","    return q_z"]},{"cell_type":"code","source":["def sample_z(q_z, num_samples):\n","    \"\"\"\n","    Sample z from q(z)\n","\n","    Parameters:\n","    q_z         : np.ndarray of shape [H, W], posterior probability of sampling at each pixel\n","    num_samples : int, number of samples\n","\n","    Returns:\n","    z_sample    : np.ndarray of shape [H, W], count of times each pixel was sampled\n","    \"\"\"\n","    H, W = q_z.shape\n","    flat_q = q_z.flatten()\n","    # flat_q /= flat_q.sum()\n","    sampled_indices = np.random.choice(len(flat_q), size=num_samples, p=flat_q)\n","    counts_flat = np.bincount(sampled_indices, minlength=H * W)\n","    z_sample = counts_flat.reshape(H, W)\n","    return z_sample"],"metadata":{"id":"4ZgiezO6lEX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def M_step(x, y, q_z, theta, lr=1e-2, steps=100):\n","    \"\"\"\n","    M-step: update theta to maximize E_q(z)[log p(y | z, x, theta)]\n","\n","    Parameters:\n","        x, y  : stimulus and observed neural response, inputs to model_lkhd\n","        q_z   : np.ndarray of shape [H, W], posterior probability over latent z\n","        theta : torch.nn.Module or torch.nn.Parameter (parameters to optimize)\n","        lr    : learning rate\n","        steps : number of gradient descent steps\n","\n","    Returns:\n","        Updated theta\n","    \"\"\"\n","    # H, W = q_z.shape\n","    # optimizer = optim.Adam(theta.parameters(), lr=lr)\n","\n","    # for step in range(steps):\n","    #     optimizer.zero_grad()\n","    #     loss = 0\n","    #     for h in range(H):\n","    #         for w in range(W):\n","    #             weight = q_z[h, w]\n","    #             if weight == 0:\n","    #                 continue\n","    #             else:\n","    #                 z = torch.tensor([h, w])\n","    #                 log_lkhd = model_log_lkhd(x, y, z, theta)\n","    #                 loss -= weight * log_lkhd\n","    #     loss.backward()\n","    #     optimizer.step()\n","    # return theta\n","\n","    H, W = q_z.shape\n","    q_z_flat = torch.tensor(q_z).flatten()\n","    h_coords, w_coords = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n","    z_grid = torch.stack([h_coords, w_coords], dim=-1).reshape(-1, 2)\n","\n","    mask = q_z_flat > 0\n","    z_grid_masked = z_grid[mask]\n","    sample_weights = q_z_flat[mask]\n","\n","    optimizer = optim.Adam(theta.parameters(), lr=lr)\n","    for step in range(steps):\n","        optimizer.zero_grad()\n","        log_lkhd = model_log_lkhd(x, y, z_grid_masked, theta)\n","        loss = - (sample_weights * log_lkhd).sum()\n","        loss.backward()\n","        optimizer.step()\n","    return theta"],"metadata":{"id":"ui-ye2E6FJ_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(0)\n","torch.manual_seed(0)\n","\n","H, W = 60, 100\n","z_true = np.array([10, 12])  # latent location\n","sigma_z_hat = 2.0\n","z_hat = z_true + np.random.randn(2) * sigma_z_hat\n","z_hat = torch.tensor(z_hat, dtype=torch.float32)\n","\n","x = np.random.randn(5)\n"],"metadata":{"id":"ceyMHrkiF2cx"},"execution_count":null,"outputs":[]}]}